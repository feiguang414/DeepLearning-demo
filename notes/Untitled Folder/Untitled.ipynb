{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02da4444",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2309a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Dataset in module torch.utils.data.dataset:\n",
      "\n",
      "class Dataset(typing.Generic)\n",
      " |  An abstract class representing a :class:`Dataset`.\n",
      " |  \n",
      " |  All datasets that represent a map from keys to data samples should subclass\n",
      " |  it. All subclasses should overwrite :meth:`__getitem__`, supporting fetching a\n",
      " |  data sample for a given key. Subclasses could also optionally overwrite\n",
      " |  :meth:`__len__`, which is expected to return the size of the dataset by many\n",
      " |  :class:`~torch.utils.data.Sampler` implementations and the default options\n",
      " |  of :class:`~torch.utils.data.DataLoader`.\n",
      " |  \n",
      " |  .. note::\n",
      " |    :class:`~torch.utils.data.DataLoader` by default constructs a index\n",
      " |    sampler that yields integral indices.  To make it work with a map-style\n",
      " |    dataset with non-integral indices/keys, a custom sampler must be provided.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Dataset\n",
      " |      typing.Generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __add__(self, other:'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      " |  \n",
      " |  __getattr__(self, attribute_name)\n",
      " |  \n",
      " |  __getitem__(self, index) -> +T_co\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  register_datapipe_as_function(function_name, cls_to_register, enable_df_api_tracing=False) from typing.GenericMeta\n",
      " |  \n",
      " |  register_function(function_name, function) from typing.GenericMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  __annotations__ = {'functions': typing.Dict[str, typing.Callable]}\n",
      " |  \n",
      " |  __args__ = None\n",
      " |  \n",
      " |  __extra__ = None\n",
      " |  \n",
      " |  __next_in_mro__ = <class 'object'>\n",
      " |      The most base type\n",
      " |  \n",
      " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
      " |  \n",
      " |  __origin__ = None\n",
      " |  \n",
      " |  __parameters__ = (+T_co,)\n",
      " |  \n",
      " |  __tree_hash__ = -9223371887804758737\n",
      " |  \n",
      " |  functions = {'concat': functools.partial(<function Dataset.register_da...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from typing.Generic:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwds)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c93345",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad63c48d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function add_scalar in module torch.utils.tensorboard.writer:\n",
      "\n",
      "add_scalar(self, tag, scalar_value, global_step=None, walltime=None, new_style=False, double_precision=False)\n",
      "    Add scalar data to summary.\n",
      "    \n",
      "    Args:\n",
      "        tag (string): Data identifier\n",
      "        scalar_value (float or string/blobname): Value to save\n",
      "        global_step (int): Global step value to record\n",
      "        walltime (float): Optional override default walltime (time.time())\n",
      "          with seconds after epoch of event\n",
      "        new_style (boolean): Whether to use new style (tensor field) or old\n",
      "          style (simple_value field). New style could lead to faster data loading.\n",
      "    Examples::\n",
      "    \n",
      "        from torch.utils.tensorboard import SummaryWriter\n",
      "        writer = SummaryWriter()\n",
      "        x = range(100)\n",
      "        for i in x:\n",
      "            writer.add_scalar('y=2x', i * 2, i)\n",
      "        writer.close()\n",
      "    \n",
      "    Expected result:\n",
      "    \n",
      "    .. image:: _static/img/tensorboard/add_scalar.png\n",
      "       :scale: 50 %\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "help(SummaryWriter.add_scalar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3980d37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module d2l.torch in d2l:\n",
      "\n",
      "NAME\n",
      "    d2l.torch\n",
      "\n",
      "DESCRIPTION\n",
      "    # This file is generated automatically through:\n",
      "    #    d2lbook build lib\n",
      "    # Don't edit it directly\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Accumulator\n",
      "        Animator\n",
      "        Benchmark\n",
      "        RNNModelScratch\n",
      "        RandomGenerator\n",
      "        SeqDataLoader\n",
      "        Timer\n",
      "        TokenEmbedding\n",
      "        Vocab\n",
      "    torch.nn.modules.loss.CrossEntropyLoss(torch.nn.modules.loss._WeightedLoss)\n",
      "        MaskedSoftmaxCELoss\n",
      "    torch.nn.modules.module.Module(builtins.object)\n",
      "        AddNorm\n",
      "        AdditiveAttention\n",
      "        BERTEncoder\n",
      "        BERTModel\n",
      "        Decoder\n",
      "            AttentionDecoder\n",
      "        DotProductAttention\n",
      "        Encoder\n",
      "            Seq2SeqEncoder\n",
      "            TransformerEncoder\n",
      "        EncoderBlock\n",
      "        EncoderDecoder\n",
      "        MaskLM\n",
      "        MultiHeadAttention\n",
      "        NextSentencePred\n",
      "        PositionWiseFFN\n",
      "        PositionalEncoding\n",
      "        RNNModel\n",
      "        Residual\n",
      "    torch.utils.data.dataset.Dataset(typing.Generic)\n",
      "        BananasDataset\n",
      "        SNLIDataset\n",
      "        VOCSegDataset\n",
      "    \n",
      "    class Accumulator(builtins.object)\n",
      "     |  For accumulating sums over `n` variables.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, n)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  add(self, *args)\n",
      "     |  \n",
      "     |  reset(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class AddNorm(torch.nn.modules.module.Module)\n",
      "     |  Residual connection followed by layer normalization.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AddNorm\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, normalized_shape, dropout, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X, Y)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class AdditiveAttention(torch.nn.modules.module.Module)\n",
      "     |  Additive attention.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AdditiveAttention\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, key_size, query_size, num_hiddens, dropout, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, queries, keys, values, valid_lens)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class Animator(builtins.object)\n",
      "     |  For plotting data in animation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1, figsize=(3.5, 2.5))\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  add(self, x, y)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class AttentionDecoder(Decoder)\n",
      "     |  The base attention-based decoder interface.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AttentionDecoder\n",
      "     |      Decoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  attention_weights\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Decoder:\n",
      "     |  \n",
      "     |  forward(self, X, state)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  init_state(self, enc_outputs, *args)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class BERTEncoder(torch.nn.modules.module.Module)\n",
      "     |  BERT encoder.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BERTEncoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, tokens, segments, valid_lens)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class BERTModel(torch.nn.modules.module.Module)\n",
      "     |  The BERT model.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BERTModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vocab_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, max_len=1000, key_size=768, query_size=768, value_size=768, hid_in_features=768, mlm_in_features=768, nsp_in_features=768)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, tokens, segments, valid_lens=None, pred_positions=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class BananasDataset(torch.utils.data.dataset.Dataset)\n",
      "     |  A customized dataset to load the banana detection dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BananasDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, is_train)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __args__ = None\n",
      "     |  \n",
      "     |  __extra__ = None\n",
      "     |  \n",
      "     |  __next_in_mro__ = <class 'object'>\n",
      "     |      The most base type\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset,)\n",
      "     |  \n",
      "     |  __origin__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __tree_hash__ = -9223371887801563089\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other:'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  __getattr__(self, attribute_name)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  register_datapipe_as_function(function_name, cls_to_register, enable_df_api_tracing=False) from typing.GenericMeta\n",
      "     |  \n",
      "     |  register_function(function_name, function) from typing.GenericMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __annotations__ = {'functions': typing.Dict[str, typing.Callable]}\n",
      "     |  \n",
      "     |  functions = {'concat': functools.partial(<function Dataset.register_da...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Benchmark(builtins.object)\n",
      "     |  For measuring running time.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __enter__(self)\n",
      "     |  \n",
      "     |  __exit__(self, *args)\n",
      "     |  \n",
      "     |  __init__(self, description='Done')\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Decoder(torch.nn.modules.module.Module)\n",
      "     |  The base decoder interface for the encoder-decoder architecture.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Decoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X, state)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  init_state(self, enc_outputs, *args)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class DotProductAttention(torch.nn.modules.module.Module)\n",
      "     |  Scaled dot product attention.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DotProductAttention\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dropout, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, queries, keys, values, valid_lens=None)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class Encoder(torch.nn.modules.module.Module)\n",
      "     |  The base encoder interface for the encoder-decoder architecture.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Encoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X, *args)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class EncoderBlock(torch.nn.modules.module.Module)\n",
      "     |  Transformer encoder block.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EncoderBlock\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, dropout, use_bias=False, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X, valid_lens)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class EncoderDecoder(torch.nn.modules.module.Module)\n",
      "     |  The base class for the encoder-decoder architecture.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      EncoderDecoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, encoder, decoder, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, enc_X, dec_X, *args)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class MaskLM(torch.nn.modules.module.Module)\n",
      "     |  The masked language model task of BERT.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaskLM\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vocab_size, num_hiddens, num_inputs=768, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X, pred_positions)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class MaskedSoftmaxCELoss(torch.nn.modules.loss.CrossEntropyLoss)\n",
      "     |  The softmax cross-entropy loss with masks.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MaskedSoftmaxCELoss\n",
      "     |      torch.nn.modules.loss.CrossEntropyLoss\n",
      "     |      torch.nn.modules.loss._WeightedLoss\n",
      "     |      torch.nn.modules.loss._Loss\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  forward(self, pred, label, valid_len)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.loss.CrossEntropyLoss:\n",
      "     |  \n",
      "     |  __init__(self, weight:Union[torch.Tensor, NoneType]=None, size_average=None, ignore_index:int=-100, reduce=None, reduction:str='mean', label_smoothing:float=0.0) -> None\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.loss.CrossEntropyLoss:\n",
      "     |  \n",
      "     |  __annotations__ = {'ignore_index': <class 'int'>, 'label_smoothing': <...\n",
      "     |  \n",
      "     |  __constants__ = ['ignore_index', 'reduction', 'label_smoothing']\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class MultiHeadAttention(torch.nn.modules.module.Module)\n",
      "     |  Multi-head attention.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MultiHeadAttention\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, key_size, query_size, value_size, num_hiddens, num_heads, dropout, bias=False, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, queries, keys, values, valid_lens)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class NextSentencePred(torch.nn.modules.module.Module)\n",
      "     |  The next sentence prediction task of BERT.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NextSentencePred\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_inputs, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PositionWiseFFN(torch.nn.modules.module.Module)\n",
      "     |  Positionwise feed-forward network.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PositionWiseFFN\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, ffn_num_input, ffn_num_hiddens, ffn_num_outputs, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class PositionalEncoding(torch.nn.modules.module.Module)\n",
      "     |  Positional encoding.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      PositionalEncoding\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, num_hiddens, dropout, max_len=1000)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class RNNModel(torch.nn.modules.module.Module)\n",
      "     |  The RNN model.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      RNNModel\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, rnn_layer, vocab_size, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  begin_state(self, device, batch_size=1)\n",
      "     |  \n",
      "     |  forward(self, inputs, state)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class RNNModelScratch(builtins.object)\n",
      "     |  A RNN Model implemented from scratch.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, X, state)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  begin_state(self, batch_size, device)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class RandomGenerator(builtins.object)\n",
      "     |  Randomly draw among {1, ..., n} according to n sampling weights.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, sampling_weights)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  draw(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Residual(torch.nn.modules.module.Module)\n",
      "     |  The Residual block of ResNet.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Residual\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, input_channels, num_channels, use_1x1conv=False, strides=1)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class SNLIDataset(torch.utils.data.dataset.Dataset)\n",
      "     |  A customized dataset to load the SNLI dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SNLIDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, dataset, num_steps, vocab=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __args__ = None\n",
      "     |  \n",
      "     |  __extra__ = None\n",
      "     |  \n",
      "     |  __next_in_mro__ = <class 'object'>\n",
      "     |      The most base type\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset,)\n",
      "     |  \n",
      "     |  __origin__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __tree_hash__ = -9223371887801561968\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other:'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  __getattr__(self, attribute_name)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  register_datapipe_as_function(function_name, cls_to_register, enable_df_api_tracing=False) from typing.GenericMeta\n",
      "     |  \n",
      "     |  register_function(function_name, function) from typing.GenericMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __annotations__ = {'functions': typing.Dict[str, typing.Callable]}\n",
      "     |  \n",
      "     |  functions = {'concat': functools.partial(<function Dataset.register_da...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Seq2SeqEncoder(Encoder)\n",
      "     |  The RNN encoder for sequence to sequence learning.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Seq2SeqEncoder\n",
      "     |      Encoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X, *args)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class SeqDataLoader(builtins.object)\n",
      "     |  An iterator to load sequence data.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, batch_size, num_steps, use_random_iter, max_tokens)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Timer(builtins.object)\n",
      "     |  Record multiple running times.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  avg(self)\n",
      "     |      Return the average time.\n",
      "     |  \n",
      "     |  cumsum(self)\n",
      "     |      Return the accumulated time.\n",
      "     |  \n",
      "     |  start(self)\n",
      "     |      Start the timer.\n",
      "     |  \n",
      "     |  stop(self)\n",
      "     |      Stop the timer and record the time in a list.\n",
      "     |  \n",
      "     |  sum(self)\n",
      "     |      Return the sum of time.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TokenEmbedding(builtins.object)\n",
      "     |  Token Embedding.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, tokens)\n",
      "     |  \n",
      "     |  __init__(self, embedding_name)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TransformerEncoder(Encoder)\n",
      "     |  Transformer encoder.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TransformerEncoder\n",
      "     |      Encoder\n",
      "     |      torch.nn.modules.module.Module\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, vocab_size, key_size, query_size, value_size, num_hiddens, norm_shape, ffn_num_input, ffn_num_hiddens, num_heads, num_layers, dropout, use_bias=False, **kwargs)\n",
      "     |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
      "     |  \n",
      "     |  forward(self, X, valid_lens, *args)\n",
      "     |      Defines the computation performed at every call.\n",
      "     |      \n",
      "     |      Should be overridden by all subclasses.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          Although the recipe for forward pass needs to be defined within\n",
      "     |          this function, one should call the :class:`Module` instance afterwards\n",
      "     |          instead of this since the former takes care of running the\n",
      "     |          registered hooks while the latter silently ignores them.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __call__ = _call_impl(self, *input, **kwargs)\n",
      "     |  \n",
      "     |  __delattr__(self, name)\n",
      "     |      Implement delattr(self, name).\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      __dir__() -> list\n",
      "     |      default dir() implementation\n",
      "     |  \n",
      "     |  __getattr__(self, name:str) -> Union[torch.Tensor, _ForwardRef('Module')]\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __setattr__(self, name:str, value:Union[torch.Tensor, _ForwardRef('Module')]) -> None\n",
      "     |      Implement setattr(self, name, value).\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  add_module(self, name:str, module:Union[_ForwardRef('Module'), NoneType]) -> None\n",
      "     |      Adds a child module to the current module.\n",
      "     |      \n",
      "     |      The module can be accessed as an attribute using the given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the child module. The child module can be\n",
      "     |              accessed from this module using the given name\n",
      "     |          module (Module): child module to be added to the module.\n",
      "     |  \n",
      "     |  apply(self:~T, fn:Callable[[_ForwardRef('Module')], NoneType]) -> ~T\n",
      "     |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
      "     |      as well as self. Typical use includes initializing the parameters of a model\n",
      "     |      (see also :ref:`nn-init-doc`).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> @torch.no_grad()\n",
      "     |          >>> def init_weights(m):\n",
      "     |          >>>     print(m)\n",
      "     |          >>>     if type(m) == nn.Linear:\n",
      "     |          >>>         m.weight.fill_(1.0)\n",
      "     |          >>>         print(m.weight)\n",
      "     |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
      "     |          >>> net.apply(init_weights)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 1.,  1.],\n",
      "     |                  [ 1.,  1.]])\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |  \n",
      "     |  bfloat16(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  buffers(self, recurse:bool=True) -> Iterator[torch.Tensor]\n",
      "     |      Returns an iterator over module buffers.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          torch.Tensor: module buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for buf in model.buffers():\n",
      "     |          >>>     print(type(buf), buf.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  children(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over immediate children modules.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a child module\n",
      "     |  \n",
      "     |  cpu(self:~T) -> ~T\n",
      "     |      Moves all model parameters and buffers to the CPU.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  cuda(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the GPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on GPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  double(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  eval(self:~T) -> ~T\n",
      "     |      Sets the module in evaluation mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.eval()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  extra_repr(self) -> str\n",
      "     |      Set the extra representation of the module\n",
      "     |      \n",
      "     |      To print customized extra information, you should re-implement\n",
      "     |      this method in your own modules. Both single-line and multi-line\n",
      "     |      strings are acceptable.\n",
      "     |  \n",
      "     |  float(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  get_buffer(self, target:str) -> 'Tensor'\n",
      "     |      Returns the buffer given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the buffer\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.Tensor: The buffer referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not a\n",
      "     |              buffer\n",
      "     |  \n",
      "     |  get_extra_state(self) -> Any\n",
      "     |      Returns any extra state to include in the module's state_dict.\n",
      "     |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
      "     |      if you need to store extra state. This function is called when building the\n",
      "     |      module's `state_dict()`.\n",
      "     |      \n",
      "     |      Note that extra state should be pickleable to ensure working serialization\n",
      "     |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
      "     |      for serializing Tensors; other objects may break backwards compatibility if\n",
      "     |      their serialized pickled form changes.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          object: Any extra state to store in the module's state_dict\n",
      "     |  \n",
      "     |  get_parameter(self, target:str) -> 'Parameter'\n",
      "     |      Returns the parameter given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      See the docstring for ``get_submodule`` for a more detailed\n",
      "     |      explanation of this method's functionality as well as how to\n",
      "     |      correctly specify ``target``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the Parameter\n",
      "     |              to look for. (See ``get_submodule`` for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Parameter``\n",
      "     |  \n",
      "     |  get_submodule(self, target:str) -> 'Module'\n",
      "     |      Returns the submodule given by ``target`` if it exists,\n",
      "     |      otherwise throws an error.\n",
      "     |      \n",
      "     |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
      "     |      looks like this:\n",
      "     |      \n",
      "     |      .. code-block::text\n",
      "     |      \n",
      "     |          A(\n",
      "     |              (net_b): Module(\n",
      "     |                  (net_c): Module(\n",
      "     |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
      "     |                  )\n",
      "     |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
      "     |              )\n",
      "     |          )\n",
      "     |      \n",
      "     |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
      "     |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
      "     |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
      "     |      \n",
      "     |      To check whether or not we have the ``linear`` submodule, we\n",
      "     |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
      "     |      we have the ``conv`` submodule, we would call\n",
      "     |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
      "     |      \n",
      "     |      The runtime of ``get_submodule`` is bounded by the degree\n",
      "     |      of module nesting in ``target``. A query against\n",
      "     |      ``named_modules`` achieves the same result, but it is O(N) in\n",
      "     |      the number of transitive modules. So, for a simple check to see\n",
      "     |      if some submodule exists, ``get_submodule`` should always be\n",
      "     |      used.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          target: The fully-qualified string name of the submodule\n",
      "     |              to look for. (See above example for how to specify a\n",
      "     |              fully-qualified string.)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          torch.nn.Module: The submodule referenced by ``target``\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AttributeError: If the target string references an invalid\n",
      "     |              path or resolves to something that is not an\n",
      "     |              ``nn.Module``\n",
      "     |  \n",
      "     |  half(self:~T) -> ~T\n",
      "     |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  load_state_dict(self, state_dict:'OrderedDict[str, Tensor]', strict:bool=True)\n",
      "     |      Copies parameters and buffers from :attr:`state_dict` into\n",
      "     |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
      "     |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
      "     |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state_dict (dict): a dict containing parameters and\n",
      "     |              persistent buffers.\n",
      "     |          strict (bool, optional): whether to strictly enforce that the keys\n",
      "     |              in :attr:`state_dict` match the keys returned by this module's\n",
      "     |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
      "     |              * **missing_keys** is a list of str containing the missing keys\n",
      "     |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
      "     |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
      "     |          ``RuntimeError``.\n",
      "     |  \n",
      "     |  modules(self) -> Iterator[_ForwardRef('Module')]\n",
      "     |      Returns an iterator over all modules in the network.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Module: a module in the network\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          )\n",
      "     |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
      "     |  \n",
      "     |  named_buffers(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.Tensor]]\n",
      "     |      Returns an iterator over module buffers, yielding both the\n",
      "     |      name of the buffer as well as the buffer itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all buffer names.\n",
      "     |          recurse (bool): if True, then yields buffers of this module\n",
      "     |              and all submodules. Otherwise, yields only buffers that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, torch.Tensor): Tuple containing the name and buffer\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, buf in self.named_buffers():\n",
      "     |          >>>    if name in ['running_var']:\n",
      "     |          >>>        print(buf.size())\n",
      "     |  \n",
      "     |  named_children(self) -> Iterator[Tuple[str, _ForwardRef('Module')]]\n",
      "     |      Returns an iterator over immediate children modules, yielding both\n",
      "     |      the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple containing a name and child module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, module in model.named_children():\n",
      "     |          >>>     if name in ['conv4', 'conv5']:\n",
      "     |          >>>         print(module)\n",
      "     |  \n",
      "     |  named_modules(self, memo:Union[Set[_ForwardRef('Module')], NoneType]=None, prefix:str='', remove_duplicate:bool=True)\n",
      "     |      Returns an iterator over all modules in the network, yielding\n",
      "     |      both the name of the module as well as the module itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          memo: a memo to store the set of modules already added to the result\n",
      "     |          prefix: a prefix that will be added to the name of the module\n",
      "     |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
      "     |          or not\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Module): Tuple of name and module\n",
      "     |      \n",
      "     |      Note:\n",
      "     |          Duplicate modules are returned only once. In the following\n",
      "     |          example, ``l`` will be returned only once.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> l = nn.Linear(2, 2)\n",
      "     |          >>> net = nn.Sequential(l, l)\n",
      "     |          >>> for idx, m in enumerate(net.named_modules()):\n",
      "     |                  print(idx, '->', m)\n",
      "     |      \n",
      "     |          0 -> ('', Sequential(\n",
      "     |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          ))\n",
      "     |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
      "     |  \n",
      "     |  named_parameters(self, prefix:str='', recurse:bool=True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
      "     |      Returns an iterator over module parameters, yielding both the\n",
      "     |      name of the parameter as well as the parameter itself.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          prefix (str): prefix to prepend to all parameter names.\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          (string, Parameter): Tuple containing the name and parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for name, param in self.named_parameters():\n",
      "     |          >>>    if name in ['bias']:\n",
      "     |          >>>        print(param.size())\n",
      "     |  \n",
      "     |  parameters(self, recurse:bool=True) -> Iterator[torch.nn.parameter.Parameter]\n",
      "     |      Returns an iterator over module parameters.\n",
      "     |      \n",
      "     |      This is typically passed to an optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          recurse (bool): if True, then yields parameters of this module\n",
      "     |              and all submodules. Otherwise, yields only parameters that\n",
      "     |              are direct members of this module.\n",
      "     |      \n",
      "     |      Yields:\n",
      "     |          Parameter: module parameter\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> for param in model.parameters():\n",
      "     |          >>>     print(type(param), param.size())\n",
      "     |          <class 'torch.Tensor'> (20L,)\n",
      "     |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
      "     |  \n",
      "     |  register_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
      "     |      the behavior of this function will change in future versions.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_buffer(self, name:str, tensor:Union[torch.Tensor, NoneType], persistent:bool=True) -> None\n",
      "     |      Adds a buffer to the module.\n",
      "     |      \n",
      "     |      This is typically used to register a buffer that should not to be\n",
      "     |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
      "     |      is not a parameter, but is part of the module's state. Buffers, by\n",
      "     |      default, are persistent and will be saved alongside parameters. This\n",
      "     |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
      "     |      only difference between a persistent buffer and a non-persistent buffer\n",
      "     |      is that the latter will not be a part of this module's\n",
      "     |      :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Buffers can be accessed as attributes using given names.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the buffer. The buffer can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
      "     |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
      "     |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
      "     |          persistent (bool): whether the buffer is part of this module's\n",
      "     |              :attr:`state_dict`.\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
      "     |  \n",
      "     |  register_forward_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time after :func:`forward` has computed an output.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input, output) -> None or modified output\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the output. It can modify the input inplace but\n",
      "     |      it will not have effect on forward since this is called after\n",
      "     |      :func:`forward` is called.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_forward_pre_hook(self, hook:Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a forward pre-hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time before :func:`forward` is invoked.\n",
      "     |      It should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, input) -> None or modified input\n",
      "     |      \n",
      "     |      The input contains only the positional arguments given to the module.\n",
      "     |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
      "     |      The hook can modify the input. User can either return a tuple or a\n",
      "     |      single modified value in the hook. We will wrap the value into a tuple\n",
      "     |      if a single value is returned(unless that value is already a tuple).\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_full_backward_hook(self, hook:Callable[[_ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
      "     |      Registers a backward hook on the module.\n",
      "     |      \n",
      "     |      The hook will be called every time the gradients with respect to module\n",
      "     |      inputs are computed. The hook should have the following signature::\n",
      "     |      \n",
      "     |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
      "     |      \n",
      "     |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
      "     |      with respect to the inputs and outputs respectively. The hook should\n",
      "     |      not modify its arguments, but it can optionally return a new gradient with\n",
      "     |      respect to the input that will be used in place of :attr:`grad_input` in\n",
      "     |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
      "     |      as positional arguments and all kwarg arguments are ignored. Entries\n",
      "     |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
      "     |      arguments.\n",
      "     |      \n",
      "     |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
      "     |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
      "     |      of each Tensor returned by the Module's forward function.\n",
      "     |      \n",
      "     |      .. warning ::\n",
      "     |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
      "     |          will raise an error.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          :class:`torch.utils.hooks.RemovableHandle`:\n",
      "     |              a handle that can be used to remove the added hook by calling\n",
      "     |              ``handle.remove()``\n",
      "     |  \n",
      "     |  register_parameter(self, name:str, param:Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
      "     |      Adds a parameter to the module.\n",
      "     |      \n",
      "     |      The parameter can be accessed as an attribute using given name.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          name (string): name of the parameter. The parameter can be accessed\n",
      "     |              from this module using the given name\n",
      "     |          param (Parameter or None): parameter to be added to the module. If\n",
      "     |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
      "     |              are ignored. If ``None``, the parameter is **not** included in the\n",
      "     |              module's :attr:`state_dict`.\n",
      "     |  \n",
      "     |  requires_grad_(self:~T, requires_grad:bool=True) -> ~T\n",
      "     |      Change if autograd should record operations on parameters in this\n",
      "     |      module.\n",
      "     |      \n",
      "     |      This method sets the parameters' :attr:`requires_grad` attributes\n",
      "     |      in-place.\n",
      "     |      \n",
      "     |      This method is helpful for freezing part of the module for finetuning\n",
      "     |      or training parts of a model individually (e.g., GAN training).\n",
      "     |      \n",
      "     |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
      "     |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          requires_grad (bool): whether autograd should record operations on\n",
      "     |                                parameters in this module. Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  set_extra_state(self, state:Any)\n",
      "     |      This function is called from :func:`load_state_dict` to handle any extra state\n",
      "     |      found within the `state_dict`. Implement this function and a corresponding\n",
      "     |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
      "     |      `state_dict`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          state (dict): Extra state from the `state_dict`\n",
      "     |  \n",
      "     |  share_memory(self:~T) -> ~T\n",
      "     |      See :meth:`torch.Tensor.share_memory_`\n",
      "     |  \n",
      "     |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
      "     |      Returns a dictionary containing a whole state of the module.\n",
      "     |      \n",
      "     |      Both parameters and persistent buffers (e.g. running averages) are\n",
      "     |      included. Keys are corresponding parameter and buffer names.\n",
      "     |      Parameters and buffers set to ``None`` are not included.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          dict:\n",
      "     |              a dictionary containing a whole state of the module\n",
      "     |      \n",
      "     |      Example::\n",
      "     |      \n",
      "     |          >>> module.state_dict().keys()\n",
      "     |          ['bias', 'weight']\n",
      "     |  \n",
      "     |  to(self, *args, **kwargs)\n",
      "     |      Moves and/or casts the parameters and buffers.\n",
      "     |      \n",
      "     |      This can be called as\n",
      "     |      \n",
      "     |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(dtype, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(tensor, non_blocking=False)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      .. function:: to(memory_format=torch.channels_last)\n",
      "     |         :noindex:\n",
      "     |      \n",
      "     |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
      "     |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
      "     |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
      "     |      (if given). The integral parameters and buffers will be moved\n",
      "     |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
      "     |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
      "     |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
      "     |      pinned memory to CUDA devices.\n",
      "     |      \n",
      "     |      See below for examples.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): the desired device of the parameters\n",
      "     |              and buffers in this module\n",
      "     |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
      "     |              the parameters and buffers in this module\n",
      "     |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
      "     |              dtype and device for all parameters and buffers in this module\n",
      "     |          memory_format (:class:`torch.memory_format`): the desired memory\n",
      "     |              format for 4D parameters and buffers in this module (keyword\n",
      "     |              only argument)\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |      \n",
      "     |      Examples::\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]])\n",
      "     |          >>> linear.to(torch.double)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1913, -0.3420],\n",
      "     |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
      "     |          >>> gpu1 = torch.device(\"cuda:1\")\n",
      "     |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
      "     |          >>> cpu = torch.device(\"cpu\")\n",
      "     |          >>> linear.to(cpu)\n",
      "     |          Linear(in_features=2, out_features=2, bias=True)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.1914, -0.3420],\n",
      "     |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
      "     |      \n",
      "     |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
      "     |          >>> linear.weight\n",
      "     |          Parameter containing:\n",
      "     |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
      "     |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
      "     |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
      "     |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j],\n",
      "     |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
      "     |  \n",
      "     |  to_empty(self:~T, *, device:Union[str, torch.device]) -> ~T\n",
      "     |      Moves the parameters and buffers to the specified device without copying storage.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          device (:class:`torch.device`): The desired device of the parameters\n",
      "     |              and buffers in this module.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  train(self:~T, mode:bool=True) -> ~T\n",
      "     |      Sets the module in training mode.\n",
      "     |      \n",
      "     |      This has any effect only on certain modules. See documentations of\n",
      "     |      particular modules for details of their behaviors in training/evaluation\n",
      "     |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
      "     |      etc.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          mode (bool): whether to set training mode (``True``) or evaluation\n",
      "     |                       mode (``False``). Default: ``True``.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  type(self:~T, dst_type:Union[torch.dtype, str]) -> ~T\n",
      "     |      Casts all parameters and buffers to :attr:`dst_type`.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          dst_type (type or string): the desired type\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  xpu(self:~T, device:Union[int, torch.device, NoneType]=None) -> ~T\n",
      "     |      Moves all model parameters and buffers to the XPU.\n",
      "     |      \n",
      "     |      This also makes associated parameters and buffers different objects. So\n",
      "     |      it should be called before constructing optimizer if the module will\n",
      "     |      live on XPU while being optimized.\n",
      "     |      \n",
      "     |      .. note::\n",
      "     |          This method modifies the module in-place.\n",
      "     |      \n",
      "     |      Arguments:\n",
      "     |          device (int, optional): if specified, all parameters will be\n",
      "     |              copied to that device\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Module: self\n",
      "     |  \n",
      "     |  zero_grad(self, set_to_none:bool=False) -> None\n",
      "     |      Sets gradients of all model parameters to zero. See similar function\n",
      "     |      under :class:`torch.optim.Optimizer` for more context.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
      "     |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
      "     |  \n",
      "     |  T_destination = ~T_destination\n",
      "     |      Type variable.\n",
      "     |      \n",
      "     |      Usage::\n",
      "     |      \n",
      "     |        T = TypeVar('T')  # Can be anything\n",
      "     |        A = TypeVar('A', str, bytes)  # Must be str or bytes\n",
      "     |      \n",
      "     |      Type variables exist primarily for the benefit of static type\n",
      "     |      checkers.  They serve as the parameters for generic types as well\n",
      "     |      as for generic function definitions.  See class Generic for more\n",
      "     |      information on generic types.  Generic functions work as follows:\n",
      "     |      \n",
      "     |        def repeat(x: T, n: int) -> List[T]:\n",
      "     |            '''Return a list containing n references to x.'''\n",
      "     |            return [x]*n\n",
      "     |      \n",
      "     |        def longest(x: A, y: A) -> A:\n",
      "     |            '''Return the longest of two strings.'''\n",
      "     |            return x if len(x) >= len(y) else y\n",
      "     |      \n",
      "     |      The latter example's signature is essentially the overloading\n",
      "     |      of (str, str) -> str and (bytes, bytes) -> bytes.  Also note\n",
      "     |      that if the arguments are instances of some subclass of str,\n",
      "     |      the return type is still plain str.\n",
      "     |      \n",
      "     |      At runtime, isinstance(x, T) and issubclass(C, T) will raise TypeError.\n",
      "     |      \n",
      "     |      Type variables defined with covariant=True or contravariant=True\n",
      "     |      can be used do declare covariant or contravariant generic types.\n",
      "     |      See PEP 484 for more details. By default generic types are invariant\n",
      "     |      in all type variables.\n",
      "     |      \n",
      "     |      Type variables can be introspected. e.g.:\n",
      "     |      \n",
      "     |        T.__name__ == 'T'\n",
      "     |        T.__constraints__ == ()\n",
      "     |        T.__covariant__ == False\n",
      "     |        T.__contravariant__ = False\n",
      "     |        A.__constraints__ == (str, bytes)\n",
      "     |  \n",
      "     |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
      "     |  \n",
      "     |  dump_patches = False\n",
      "    \n",
      "    class VOCSegDataset(torch.utils.data.dataset.Dataset)\n",
      "     |  A customized dataset to load the VOC dataset.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      VOCSegDataset\n",
      "     |      torch.utils.data.dataset.Dataset\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, idx)\n",
      "     |  \n",
      "     |  __init__(self, is_train, crop_size, voc_dir)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  filter(self, imgs)\n",
      "     |  \n",
      "     |  normalize_image(self, img)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __args__ = None\n",
      "     |  \n",
      "     |  __extra__ = None\n",
      "     |  \n",
      "     |  __next_in_mro__ = <class 'object'>\n",
      "     |      The most base type\n",
      "     |  \n",
      "     |  __orig_bases__ = (torch.utils.data.dataset.Dataset,)\n",
      "     |  \n",
      "     |  __origin__ = None\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  __tree_hash__ = -9223371887801563030\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __add__(self, other:'Dataset[T_co]') -> 'ConcatDataset[T_co]'\n",
      "     |  \n",
      "     |  __getattr__(self, attribute_name)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  register_datapipe_as_function(function_name, cls_to_register, enable_df_api_tracing=False) from typing.GenericMeta\n",
      "     |  \n",
      "     |  register_function(function_name, function) from typing.GenericMeta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from torch.utils.data.dataset.Dataset:\n",
      "     |  \n",
      "     |  __annotations__ = {'functions': typing.Dict[str, typing.Callable]}\n",
      "     |  \n",
      "     |  functions = {'concat': functools.partial(<function Dataset.register_da...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __new__(cls, *args, **kwds)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "    \n",
      "    class Vocab(builtins.object)\n",
      "     |  Vocabulary for text.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, tokens)\n",
      "     |  \n",
      "     |  __init__(self, tokens=None, min_freq=0, reserved_tokens=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  to_tokens(self, indices)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  token_freqs\n",
      "     |  \n",
      "     |  unk\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(...)\n",
      "        abs(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Computes the absolute value of each element in :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            \\text{out}_{i} = |\\text{input}_{i}|\n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.abs(torch.tensor([-1, -2, 3]))\n",
      "            tensor([ 1,  2,  3])\n",
      "    \n",
      "    accuracy(y_hat, y)\n",
      "        Compute the number of correct predictions.\n",
      "    \n",
      "    annotate(text, xy, xytext)\n",
      "        # Defined in file: ./chapter_optimization/optimization-intro.md\n",
      "    \n",
      "    arange(...)\n",
      "        arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "        \n",
      "        Returns a 1-D tensor of size :math:`\\left\\lceil \\frac{\\text{end} - \\text{start}}{\\text{step}} \\right\\rceil`\n",
      "        with values from the interval ``[start, end)`` taken with common difference\n",
      "        :attr:`step` beginning from `start`.\n",
      "        \n",
      "        Note that non-integer :attr:`step` is subject to floating point rounding errors when\n",
      "        comparing against :attr:`end`; to avoid inconsistency, we advise adding a small epsilon to :attr:`end`\n",
      "        in such cases.\n",
      "        \n",
      "        .. math::\n",
      "            \\text{out}_{{i+1}} = \\text{out}_{i} + \\text{step}\n",
      "        \n",
      "        Args:\n",
      "            start (Number): the starting value for the set of points. Default: ``0``.\n",
      "            end (Number): the ending value for the set of points\n",
      "            step (Number): the gap between each pair of adjacent points. Default: ``1``.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`). If `dtype` is not given, infer the data type from the other input\n",
      "                arguments. If any of `start`, `end`, or `stop` are floating-point, the\n",
      "                `dtype` is inferred to be the default dtype, see\n",
      "                :meth:`~torch.get_default_dtype`. Otherwise, the `dtype` is inferred to\n",
      "                be `torch.int64`.\n",
      "            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "                Default: ``torch.strided``.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.arange(5)\n",
      "            tensor([ 0,  1,  2,  3,  4])\n",
      "            >>> torch.arange(1, 4)\n",
      "            tensor([ 1,  2,  3])\n",
      "            >>> torch.arange(1, 2.5, 0.5)\n",
      "            tensor([ 1.0000,  1.5000,  2.0000])\n",
      "    \n",
      "    argmax lambda x, *args, **kwargs\n",
      "    \n",
      "    assign_anchor_to_bbox(ground_truth, anchors, device, iou_threshold=0.5)\n",
      "        Assign closest ground-truth bounding boxes to anchor boxes.\n",
      "    \n",
      "    astype lambda x, *args, **kwargs\n",
      "    \n",
      "    batchify(data)\n",
      "        Return a minibatch of examples for skip-gram with negative sampling.\n",
      "    \n",
      "    bbox_to_rect(bbox, color)\n",
      "        Convert bounding box to matplotlib format.\n",
      "    \n",
      "    bleu(pred_seq, label_seq, k)\n",
      "        Compute the BLEU.\n",
      "    \n",
      "    box_center_to_corner(boxes)\n",
      "        Convert from (center, width, height) to (upper-left, lower-right).\n",
      "    \n",
      "    box_corner_to_center(boxes)\n",
      "        Convert from (upper-left, lower-right) to (center, width, height).\n",
      "    \n",
      "    box_iou(boxes1, boxes2)\n",
      "        Compute pairwise IoU across two lists of anchor or bounding boxes.\n",
      "    \n",
      "    build_array_nmt(lines, vocab, num_steps)\n",
      "        Transform text sequences of machine translation into minibatches.\n",
      "    \n",
      "    concat = cat(...)\n",
      "        cat(tensors, dim=0, *, out=None) -> Tensor\n",
      "        \n",
      "        Concatenates the given sequence of :attr:`seq` tensors in the given dimension.\n",
      "        All tensors must either have the same shape (except in the concatenating\n",
      "        dimension) or be empty.\n",
      "        \n",
      "        :func:`torch.cat` can be seen as an inverse operation for :func:`torch.split`\n",
      "        and :func:`torch.chunk`.\n",
      "        \n",
      "        :func:`torch.cat` can be best understood via examples.\n",
      "        \n",
      "        Args:\n",
      "            tensors (sequence of Tensors): any python sequence of tensors of the same type.\n",
      "                Non-empty tensors provided must have the same shape, except in the\n",
      "                cat dimension.\n",
      "            dim (int, optional): the dimension over which the tensors are concatenated\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> x = torch.randn(2, 3)\n",
      "            >>> x\n",
      "            tensor([[ 0.6580, -1.0969, -0.4614],\n",
      "                    [-0.1034, -0.5790,  0.1497]])\n",
      "            >>> torch.cat((x, x, x), 0)\n",
      "            tensor([[ 0.6580, -1.0969, -0.4614],\n",
      "                    [-0.1034, -0.5790,  0.1497],\n",
      "                    [ 0.6580, -1.0969, -0.4614],\n",
      "                    [-0.1034, -0.5790,  0.1497],\n",
      "                    [ 0.6580, -1.0969, -0.4614],\n",
      "                    [-0.1034, -0.5790,  0.1497]])\n",
      "            >>> torch.cat((x, x, x), 1)\n",
      "            tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,\n",
      "                     -1.0969, -0.4614],\n",
      "                    [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,\n",
      "                     -0.5790,  0.1497]])\n",
      "    \n",
      "    copyfile(filename, target_dir)\n",
      "        Copy a file into a target directory.\n",
      "    \n",
      "    corr2d(X, K)\n",
      "        Compute 2D cross-correlation.\n",
      "    \n",
      "    cos(...)\n",
      "        cos(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Returns a new tensor with the cosine  of the elements of :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            \\text{out}_{i} = \\cos(\\text{input}_{i})\n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> a = torch.randn(4)\n",
      "            >>> a\n",
      "            tensor([ 1.4309,  1.2706, -0.8562,  0.9796])\n",
      "            >>> torch.cos(a)\n",
      "            tensor([ 0.1395,  0.2957,  0.6553,  0.5574])\n",
      "    \n",
      "    cosh(...)\n",
      "        cosh(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Returns a new tensor with the hyperbolic cosine  of the elements of\n",
      "        :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            \\text{out}_{i} = \\cosh(\\text{input}_{i})\n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> a = torch.randn(4)\n",
      "            >>> a\n",
      "            tensor([ 0.1632,  1.1835, -0.6979, -0.7325])\n",
      "            >>> torch.cosh(a)\n",
      "            tensor([ 1.0133,  1.7860,  1.2536,  1.2805])\n",
      "        \n",
      "        .. note::\n",
      "           When :attr:`input` is on the CPU, the implementation of torch.cosh may use\n",
      "           the Sleef library, which rounds very large results to infinity or negative\n",
      "           infinity. See `here <https://sleef.org/purec.xhtml>`_ for details.\n",
      "    \n",
      "    count_corpus(tokens)\n",
      "        Count token frequencies.\n",
      "    \n",
      "    download(name, cache_dir='..\\\\data')\n",
      "        Download a file inserted into DATA_HUB, return the local filename.\n",
      "    \n",
      "    download_all()\n",
      "        Download all files in the DATA_HUB.\n",
      "    \n",
      "    download_extract(name, folder=None)\n",
      "        Download and extract a zip/tar file.\n",
      "    \n",
      "    evaluate_accuracy(net, data_iter)\n",
      "        Compute the accuracy for a model on a dataset.\n",
      "    \n",
      "    evaluate_accuracy_gpu(net, data_iter, device=None)\n",
      "        Compute the accuracy for a model on a dataset using a GPU.\n",
      "    \n",
      "    evaluate_loss(net, data_iter, loss)\n",
      "        Evaluate the loss of a model on the given dataset.\n",
      "    \n",
      "    exp(...)\n",
      "        exp(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Returns a new tensor with the exponential of the elements\n",
      "        of the input tensor :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            y_{i} = e^{x_{i}}\n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.exp(torch.tensor([0, math.log(2.)]))\n",
      "            tensor([ 1.,  2.])\n",
      "    \n",
      "    eye(...)\n",
      "        eye(n, m=None, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "        \n",
      "        Returns a 2-D tensor with ones on the diagonal and zeros elsewhere.\n",
      "        \n",
      "        Args:\n",
      "            n (int): the number of rows\n",
      "            m (int, optional): the number of columns with default being :attr:`n`\n",
      "        \n",
      "        Keyword arguments:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "                Default: ``torch.strided``.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "        \n",
      "        Returns:\n",
      "            Tensor: A 2-D tensor with ones on the diagonal and zeros elsewhere\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.eye(3)\n",
      "            tensor([[ 1.,  0.,  0.],\n",
      "                    [ 0.,  1.,  0.],\n",
      "                    [ 0.,  0.,  1.]])\n",
      "    \n",
      "    get_centers_and_contexts(corpus, max_window_size)\n",
      "        Return center words and context words in skip-gram.\n",
      "    \n",
      "    get_data_ch11(batch_size=10, n=1500)\n",
      "    \n",
      "    get_dataloader_workers()\n",
      "        Use 4 processes to read the data.\n",
      "    \n",
      "    get_fashion_mnist_labels(labels)\n",
      "        Return text labels for the Fashion-MNIST dataset.\n",
      "    \n",
      "    get_negatives(all_contexts, vocab, counter, K)\n",
      "        Return noise words in negative sampling.\n",
      "    \n",
      "    get_tokens_and_segments(tokens_a, tokens_b=None)\n",
      "        Get tokens of the BERT input sequence and their segment IDs.\n",
      "    \n",
      "    grad_clipping(net, theta)\n",
      "        Clip the gradient.\n",
      "    \n",
      "    linreg(X, w, b)\n",
      "        The linear regression model.\n",
      "    \n",
      "    linspace(...)\n",
      "        linspace(start, end, steps, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "        \n",
      "        Creates a one-dimensional tensor of size :attr:`steps` whose values are evenly\n",
      "        spaced from :attr:`start` to :attr:`end`, inclusive. That is, the value are:\n",
      "        \n",
      "        .. math::\n",
      "            (\\text{start},\n",
      "            \\text{start} + \\frac{\\text{end} - \\text{start}}{\\text{steps} - 1},\n",
      "            \\ldots,\n",
      "            \\text{start} + (\\text{steps} - 2) * \\frac{\\text{end} - \\text{start}}{\\text{steps} - 1},\n",
      "            \\text{end})\n",
      "        \n",
      "        \n",
      "        .. warning::\n",
      "            Not providing a value for :attr:`steps` is deprecated. For backwards\n",
      "            compatibility, not providing a value for :attr:`steps` will create a tensor\n",
      "            with 100 elements. Note that this behavior is not reflected in the\n",
      "            documented function signature and should not be relied on. In a future\n",
      "            PyTorch release, failing to provide a value for :attr:`steps` will throw a\n",
      "            runtime error.\n",
      "        \n",
      "        Args:\n",
      "            start (float): the starting value for the set of points\n",
      "            end (float): the ending value for the set of points\n",
      "            steps (int): size of the constructed tensor\n",
      "        \n",
      "        Keyword arguments:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "            dtype (torch.dtype, optional): the data type to perform the computation in.\n",
      "                Default: if None, uses the global default dtype (see torch.get_default_dtype())\n",
      "                when both :attr:`start` and :attr:`end` are real,\n",
      "                and corresponding complex dtype when either is complex.\n",
      "            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "                Default: ``torch.strided``.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "        \n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.linspace(3, 10, steps=5)\n",
      "            tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])\n",
      "            >>> torch.linspace(-10, 10, steps=5)\n",
      "            tensor([-10.,  -5.,   0.,   5.,  10.])\n",
      "            >>> torch.linspace(start=-10, end=10, steps=5)\n",
      "            tensor([-10.,  -5.,   0.,   5.,  10.])\n",
      "            >>> torch.linspace(start=-10, end=10, steps=1)\n",
      "            tensor([-10.])\n",
      "    \n",
      "    load_array(data_arrays, batch_size, is_train=True)\n",
      "        Construct a PyTorch data iterator.\n",
      "    \n",
      "    load_corpus_time_machine(max_tokens=-1)\n",
      "        Return token indices and the vocabulary of the time machine dataset.\n",
      "    \n",
      "    load_data_bananas(batch_size)\n",
      "        Load the banana detection dataset.\n",
      "    \n",
      "    load_data_fashion_mnist(batch_size, resize=None)\n",
      "        Download the Fashion-MNIST dataset and then load it into memory.\n",
      "    \n",
      "    load_data_imdb(batch_size, num_steps=500)\n",
      "        Return data iterators and the vocabulary of the IMDb review dataset.\n",
      "    \n",
      "    load_data_nmt(batch_size, num_steps, num_examples=600)\n",
      "        Return the iterator and the vocabularies of the translation dataset.\n",
      "    \n",
      "    load_data_ptb(batch_size, max_window_size, num_noise_words)\n",
      "        Download the PTB dataset and then load it into memory.\n",
      "    \n",
      "    load_data_snli(batch_size, num_steps=50)\n",
      "        Download the SNLI dataset and return data iterators and vocabulary.\n",
      "    \n",
      "    load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000)\n",
      "        Return the iterator and the vocabulary of the time machine dataset.\n",
      "    \n",
      "    load_data_voc(batch_size, crop_size)\n",
      "        Load the VOC semantic segmentation dataset.\n",
      "    \n",
      "    load_data_wiki(batch_size, max_len)\n",
      "        Load the WikiText-2 dataset.\n",
      "    \n",
      "    log(...)\n",
      "        log(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Returns a new tensor with the natural logarithm of the elements\n",
      "        of :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            y_{i} = \\log_{e} (x_{i})\n",
      "        \n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> a = torch.randn(5)\n",
      "            >>> a\n",
      "            tensor([-0.7168, -0.5471, -0.8933, -1.4428, -0.1190])\n",
      "            >>> torch.log(a)\n",
      "            tensor([ nan,  nan,  nan,  nan,  nan])\n",
      "    \n",
      "    masked_softmax(X, valid_lens)\n",
      "        Perform softmax operation by masking elements on the last axis.\n",
      "    \n",
      "    matmul(...)\n",
      "        matmul(input, other, *, out=None) -> Tensor\n",
      "        \n",
      "        Matrix product of two tensors.\n",
      "        \n",
      "        The behavior depends on the dimensionality of the tensors as follows:\n",
      "        \n",
      "        - If both tensors are 1-dimensional, the dot product (scalar) is returned.\n",
      "        - If both arguments are 2-dimensional, the matrix-matrix product is returned.\n",
      "        - If the first argument is 1-dimensional and the second argument is 2-dimensional,\n",
      "          a 1 is prepended to its dimension for the purpose of the matrix multiply.\n",
      "          After the matrix multiply, the prepended dimension is removed.\n",
      "        - If the first argument is 2-dimensional and the second argument is 1-dimensional,\n",
      "          the matrix-vector product is returned.\n",
      "        - If both arguments are at least 1-dimensional and at least one argument is\n",
      "          N-dimensional (where N > 2), then a batched matrix multiply is returned.  If the first\n",
      "          argument is 1-dimensional, a 1 is prepended to its dimension for the purpose of the\n",
      "          batched matrix multiply and removed after.  If the second argument is 1-dimensional, a\n",
      "          1 is appended to its dimension for the purpose of the batched matrix multiple and removed after.\n",
      "          The non-matrix (i.e. batch) dimensions are :ref:`broadcasted <broadcasting-semantics>` (and thus\n",
      "          must be broadcastable).  For example, if :attr:`input` is a\n",
      "          :math:`(j \\times 1 \\times n \\times n)` tensor and :attr:`other` is a :math:`(k \\times n \\times n)`\n",
      "          tensor, :attr:`out` will be a :math:`(j \\times k \\times n \\times n)` tensor.\n",
      "        \n",
      "          Note that the broadcasting logic only looks at the batch dimensions when determining if the inputs\n",
      "          are broadcastable, and not the matrix dimensions. For example, if :attr:`input` is a\n",
      "          :math:`(j \\times 1 \\times n \\times m)` tensor and :attr:`other` is a :math:`(k \\times m \\times p)`\n",
      "          tensor, these inputs are valid for broadcasting even though the final two dimensions (i.e. the\n",
      "          matrix dimensions) are different. :attr:`out` will be a :math:`(j \\times k \\times n \\times p)` tensor.\n",
      "        \n",
      "        This operator supports :ref:`TensorFloat32<tf32_on_ampere>`.\n",
      "        \n",
      "        .. note::\n",
      "        \n",
      "            The 1-dimensional dot product version of this function does not support an :attr:`out` parameter.\n",
      "        \n",
      "        Arguments:\n",
      "            input (Tensor): the first tensor to be multiplied\n",
      "            other (Tensor): the second tensor to be multiplied\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> # vector x vector\n",
      "            >>> tensor1 = torch.randn(3)\n",
      "            >>> tensor2 = torch.randn(3)\n",
      "            >>> torch.matmul(tensor1, tensor2).size()\n",
      "            torch.Size([])\n",
      "            >>> # matrix x vector\n",
      "            >>> tensor1 = torch.randn(3, 4)\n",
      "            >>> tensor2 = torch.randn(4)\n",
      "            >>> torch.matmul(tensor1, tensor2).size()\n",
      "            torch.Size([3])\n",
      "            >>> # batched matrix x broadcasted vector\n",
      "            >>> tensor1 = torch.randn(10, 3, 4)\n",
      "            >>> tensor2 = torch.randn(4)\n",
      "            >>> torch.matmul(tensor1, tensor2).size()\n",
      "            torch.Size([10, 3])\n",
      "            >>> # batched matrix x batched matrix\n",
      "            >>> tensor1 = torch.randn(10, 3, 4)\n",
      "            >>> tensor2 = torch.randn(10, 4, 5)\n",
      "            >>> torch.matmul(tensor1, tensor2).size()\n",
      "            torch.Size([10, 3, 5])\n",
      "            >>> # batched matrix x broadcasted matrix\n",
      "            >>> tensor1 = torch.randn(10, 3, 4)\n",
      "            >>> tensor2 = torch.randn(4, 5)\n",
      "            >>> torch.matmul(tensor1, tensor2).size()\n",
      "            torch.Size([10, 3, 5])\n",
      "    \n",
      "    multibox_detection(cls_probs, offset_preds, anchors, nms_threshold=0.5, pos_threshold=0.009999999)\n",
      "        Predict bounding boxes using non-maximum suppression.\n",
      "    \n",
      "    multibox_prior(data, sizes, ratios)\n",
      "        Generate anchor boxes with different shapes centered on each pixel.\n",
      "    \n",
      "    multibox_target(anchors, labels)\n",
      "        Label anchor boxes using ground-truth bounding boxes.\n",
      "    \n",
      "    nms(boxes, scores, iou_threshold)\n",
      "        Sort confidence scores of predicted bounding boxes.\n",
      "    \n",
      "    normal(...)\n",
      "        normal(mean, std, *, generator=None, out=None) -> Tensor\n",
      "        \n",
      "        Returns a tensor of random numbers drawn from separate normal distributions\n",
      "        whose mean and standard deviation are given.\n",
      "        \n",
      "        The :attr:`mean` is a tensor with the mean of\n",
      "        each output element's normal distribution\n",
      "        \n",
      "        The :attr:`std` is a tensor with the standard deviation of\n",
      "        each output element's normal distribution\n",
      "        \n",
      "        The shapes of :attr:`mean` and :attr:`std` don't need to match, but the\n",
      "        total number of elements in each tensor need to be the same.\n",
      "        \n",
      "        .. note:: When the shapes do not match, the shape of :attr:`mean`\n",
      "                  is used as the shape for the returned output tensor\n",
      "        \n",
      "        .. note:: When :attr:`std` is a CUDA tensor, this function synchronizes\n",
      "                  its device with the CPU.\n",
      "        \n",
      "        Args:\n",
      "            mean (Tensor): the tensor of per-element means\n",
      "            std (Tensor): the tensor of per-element standard deviations\n",
      "        \n",
      "        Keyword args:\n",
      "            generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))\n",
      "            tensor([  1.0425,   3.5672,   2.7969,   4.2925,   4.7229,   6.2134,\n",
      "                      8.0505,   8.1408,   9.0563,  10.0566])\n",
      "        \n",
      "        .. function:: normal(mean=0.0, std, *, out=None) -> Tensor\n",
      "           :noindex:\n",
      "        \n",
      "        Similar to the function above, but the means are shared among all drawn\n",
      "        elements.\n",
      "        \n",
      "        Args:\n",
      "            mean (float, optional): the mean for all distributions\n",
      "            std (Tensor): the tensor of per-element standard deviations\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.normal(mean=0.5, std=torch.arange(1., 6.))\n",
      "            tensor([-1.2793, -1.0732, -2.0687,  5.1177, -1.2303])\n",
      "        \n",
      "        .. function:: normal(mean, std=1.0, *, out=None) -> Tensor\n",
      "           :noindex:\n",
      "        \n",
      "        Similar to the function above, but the standard deviations are shared among\n",
      "        all drawn elements.\n",
      "        \n",
      "        Args:\n",
      "            mean (Tensor): the tensor of per-element means\n",
      "            std (float, optional): the standard deviation for all distributions\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.normal(mean=torch.arange(1., 6.))\n",
      "            tensor([ 1.1552,  2.6148,  2.6535,  5.8318,  4.2361])\n",
      "        \n",
      "        .. function:: normal(mean, std, size, *, out=None) -> Tensor\n",
      "           :noindex:\n",
      "        \n",
      "        Similar to the function above, but the means and standard deviations are shared\n",
      "        among all drawn elements. The resulting tensor has size given by :attr:`size`.\n",
      "        \n",
      "        Args:\n",
      "            mean (float): the mean for all distributions\n",
      "            std (float): the standard deviation for all distributions\n",
      "            size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.normal(2, 3, size=(1, 4))\n",
      "            tensor([[-1.3987, -1.9544,  3.6048,  0.7909]])\n",
      "    \n",
      "    numpy lambda x, *args, **kwargs\n",
      "    \n",
      "    offset_boxes(anchors, assigned_bb, eps=1e-06)\n",
      "        Transform for anchor box offsets.\n",
      "    \n",
      "    offset_inverse(anchors, offset_preds)\n",
      "        Predict bounding boxes based on anchor boxes with predicted offsets.\n",
      "    \n",
      "    ones(...)\n",
      "        ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "        \n",
      "        Returns a tensor filled with the scalar value `1`, with the shape defined\n",
      "        by the variable argument :attr:`size`.\n",
      "        \n",
      "        Args:\n",
      "            size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "                Can be a variable number of arguments or a collection like a list or tuple.\n",
      "        \n",
      "        Keyword arguments:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "                Default: ``torch.strided``.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.ones(2, 3)\n",
      "            tensor([[ 1.,  1.,  1.],\n",
      "                    [ 1.,  1.,  1.]])\n",
      "        \n",
      "            >>> torch.ones(5)\n",
      "            tensor([ 1.,  1.,  1.,  1.,  1.])\n",
      "    \n",
      "    plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale='linear', yscale='linear', fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None)\n",
      "        Plot data points.\n",
      "    \n",
      "    predict_ch3(net, test_iter, n=6)\n",
      "        Predict labels (defined in Chapter 3).\n",
      "    \n",
      "    predict_ch8(prefix, num_preds, net, vocab, device)\n",
      "        Generate new characters following the `prefix`.\n",
      "    \n",
      "    predict_sentiment(net, vocab, sequence)\n",
      "        Predict the sentiment of a text sequence.\n",
      "    \n",
      "    predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, num_steps, device, save_attention_weights=False)\n",
      "        Predict for sequence to sequence.\n",
      "    \n",
      "    predict_snli(net, vocab, premise, hypothesis)\n",
      "        Predict the logical relationship between the premise and hypothesis.\n",
      "    \n",
      "    preprocess_nmt(text)\n",
      "        Preprocess the English-French dataset.\n",
      "    \n",
      "    rand(...)\n",
      "        rand(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "        \n",
      "        Returns a tensor filled with random numbers from a uniform distribution\n",
      "        on the interval :math:`[0, 1)`\n",
      "        \n",
      "        The shape of the tensor is defined by the variable argument :attr:`size`.\n",
      "        \n",
      "        Args:\n",
      "            size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "                Can be a variable number of arguments or a collection like a list or tuple.\n",
      "        \n",
      "        Keyword args:\n",
      "            generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling\n",
      "            out (Tensor, optional): the output tensor.\n",
      "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "                Default: ``torch.strided``.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.rand(4)\n",
      "            tensor([ 0.5204,  0.2503,  0.3525,  0.5673])\n",
      "            >>> torch.rand(2, 3)\n",
      "            tensor([[ 0.8237,  0.5781,  0.6879],\n",
      "                    [ 0.3816,  0.7249,  0.0998]])\n",
      "    \n",
      "    read_csv_labels(fname)\n",
      "        Read `fname` to return a filename to label dictionary.\n",
      "    \n",
      "    read_data_bananas(is_train=True)\n",
      "        Read the banana detection dataset images and labels.\n",
      "    \n",
      "    read_data_nmt()\n",
      "        Load the English-French dataset.\n",
      "    \n",
      "    read_imdb(data_dir, is_train)\n",
      "        Read the IMDb review dataset text sequences and labels.\n",
      "    \n",
      "    read_ptb()\n",
      "        Load the PTB dataset into a list of text lines.\n",
      "    \n",
      "    read_snli(data_dir, is_train)\n",
      "        Read the SNLI dataset into premises, hypotheses, and labels.\n",
      "    \n",
      "    read_time_machine()\n",
      "        Load the time machine dataset into a list of text lines.\n",
      "    \n",
      "    read_voc_images(voc_dir, is_train=True)\n",
      "        Read all VOC feature and label images.\n",
      "    \n",
      "    reduce_sum lambda x, *args, **kwargs\n",
      "    \n",
      "    reorg_test(data_dir)\n",
      "        Organize the testing set for data loading during prediction.\n",
      "    \n",
      "    reorg_train_valid(data_dir, labels, valid_ratio)\n",
      "        Split the validation set out of the original training set.\n",
      "    \n",
      "    reshape lambda x, *args, **kwargs\n",
      "    \n",
      "    resnet18(num_classes, in_channels=1)\n",
      "        A slightly modified ResNet-18 model.\n",
      "    \n",
      "    seq_data_iter_random(corpus, batch_size, num_steps)\n",
      "        Generate a minibatch of subsequences using random sampling.\n",
      "    \n",
      "    seq_data_iter_sequential(corpus, batch_size, num_steps)\n",
      "        Generate a minibatch of subsequences using sequential partitioning.\n",
      "    \n",
      "    sequence_mask(X, valid_len, value=0)\n",
      "        Mask irrelevant entries in sequences.\n",
      "    \n",
      "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
      "        Set the axes for matplotlib.\n",
      "    \n",
      "    set_figsize(figsize=(3.5, 2.5))\n",
      "        Set the figure size for matplotlib.\n",
      "    \n",
      "    sgd(params, lr, batch_size)\n",
      "        Minibatch stochastic gradient descent.\n",
      "    \n",
      "    show_bboxes(axes, bboxes, labels=None, colors=None)\n",
      "        Show bounding boxes.\n",
      "    \n",
      "    show_heatmaps(matrices, xlabel, ylabel, titles=None, figsize=(2.5, 2.5), cmap='Reds')\n",
      "        Show heatmaps of matrices.\n",
      "    \n",
      "    show_images(imgs, num_rows, num_cols, titles=None, scale=1.5)\n",
      "        Plot a list of images.\n",
      "    \n",
      "    show_list_len_pair_hist(legend, xlabel, ylabel, xlist, ylist)\n",
      "        Plot the histogram for list length pairs.\n",
      "    \n",
      "    show_trace_2d(f, results)\n",
      "        Show the trace of 2D variables during optimization.\n",
      "    \n",
      "    sin(...)\n",
      "        sin(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Returns a new tensor with the sine of the elements of :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            \\text{out}_{i} = \\sin(\\text{input}_{i})\n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> a = torch.randn(4)\n",
      "            >>> a\n",
      "            tensor([-0.5461,  0.1347, -2.7266, -0.2746])\n",
      "            >>> torch.sin(a)\n",
      "            tensor([-0.5194,  0.1343, -0.4032, -0.2711])\n",
      "    \n",
      "    sinh(...)\n",
      "        sinh(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Returns a new tensor with the hyperbolic sine of the elements of\n",
      "        :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            \\text{out}_{i} = \\sinh(\\text{input}_{i})\n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> a = torch.randn(4)\n",
      "            >>> a\n",
      "            tensor([ 0.5380, -0.8632, -0.1265,  0.9399])\n",
      "            >>> torch.sinh(a)\n",
      "            tensor([ 0.5644, -0.9744, -0.1268,  1.0845])\n",
      "        \n",
      "        .. note::\n",
      "           When :attr:`input` is on the CPU, the implementation of torch.sinh may use\n",
      "           the Sleef library, which rounds very large results to infinity or negative\n",
      "           infinity. See `here <https://sleef.org/purec.xhtml>`_ for details.\n",
      "    \n",
      "    size lambda x, *args, **kwargs\n",
      "    \n",
      "    split_batch(X, y, devices)\n",
      "        Split `X` and `y` into multiple devices.\n",
      "    \n",
      "    squared_loss(y_hat, y)\n",
      "        Squared loss.\n",
      "    \n",
      "    stack(...)\n",
      "        stack(tensors, dim=0, *, out=None) -> Tensor\n",
      "        \n",
      "        Concatenates a sequence of tensors along a new dimension.\n",
      "        \n",
      "        All tensors need to be of the same size.\n",
      "        \n",
      "        Arguments:\n",
      "            tensors (sequence of Tensors): sequence of tensors to concatenate\n",
      "            dim (int): dimension to insert. Has to be between 0 and the number\n",
      "                of dimensions of concatenated tensors (inclusive)\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "    \n",
      "    subsample(sentences, vocab)\n",
      "        Subsample high-frequency words.\n",
      "    \n",
      "    synthetic_data(w, b, num_examples)\n",
      "        Generate y = Xw + b + noise.\n",
      "    \n",
      "    tanh(...)\n",
      "        tanh(input, *, out=None) -> Tensor\n",
      "        \n",
      "        Returns a new tensor with the hyperbolic tangent of the elements\n",
      "        of :attr:`input`.\n",
      "        \n",
      "        .. math::\n",
      "            \\text{out}_{i} = \\tanh(\\text{input}_{i})\n",
      "        \n",
      "        Args:\n",
      "            input (Tensor): the input tensor.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> a = torch.randn(4)\n",
      "            >>> a\n",
      "            tensor([ 0.8986, -0.7279,  1.1745,  0.2611])\n",
      "            >>> torch.tanh(a)\n",
      "            tensor([ 0.7156, -0.6218,  0.8257,  0.2553])\n",
      "    \n",
      "    tensor(...)\n",
      "        tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) -> Tensor\n",
      "        \n",
      "        Constructs a tensor with :attr:`data`.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            :func:`torch.tensor` always copies :attr:`data`. If you have a Tensor\n",
      "            ``data`` and want to avoid a copy, use :func:`torch.Tensor.requires_grad_`\n",
      "            or :func:`torch.Tensor.detach`.\n",
      "            If you have a NumPy ``ndarray`` and want to avoid a copy, use\n",
      "            :func:`torch.as_tensor`.\n",
      "        \n",
      "        .. warning::\n",
      "        \n",
      "            When data is a tensor `x`, :func:`torch.tensor` reads out 'the data' from whatever it is passed,\n",
      "            and constructs a leaf variable. Therefore ``torch.tensor(x)`` is equivalent to ``x.clone().detach()``\n",
      "            and ``torch.tensor(x, requires_grad=True)`` is equivalent to ``x.clone().detach().requires_grad_(True)``.\n",
      "            The equivalents using ``clone()`` and ``detach()`` are recommended.\n",
      "        \n",
      "        Args:\n",
      "            data (array_like): Initial data for the tensor. Can be a list, tuple,\n",
      "                NumPy ``ndarray``, scalar, and other types.\n",
      "        \n",
      "        Keyword args:\n",
      "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "                Default: if ``None``, infers data type from :attr:`data`.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "            pin_memory (bool, optional): If set, returned tensor would be allocated in\n",
      "                the pinned memory. Works only for CPU tensors. Default: ``False``.\n",
      "        \n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
      "            tensor([[ 0.1000,  1.2000],\n",
      "                    [ 2.2000,  3.1000],\n",
      "                    [ 4.9000,  5.2000]])\n",
      "        \n",
      "            >>> torch.tensor([0, 1])  # Type inference on data\n",
      "            tensor([ 0,  1])\n",
      "        \n",
      "            >>> torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
      "            ...              dtype=torch.float64,\n",
      "            ...              device=torch.device('cuda:0'))  # creates a torch.cuda.DoubleTensor\n",
      "            tensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')\n",
      "        \n",
      "            >>> torch.tensor(3.14159)  # Create a scalar (zero-dimensional tensor)\n",
      "            tensor(3.1416)\n",
      "        \n",
      "            >>> torch.tensor([])  # Create an empty tensor (of size (0,))\n",
      "            tensor([])\n",
      "    \n",
      "    to lambda x, *args, **kwargs\n",
      "    \n",
      "    tokenize(lines, token='word')\n",
      "        Split text lines into word or character tokens.\n",
      "    \n",
      "    tokenize_nmt(text, num_examples=None)\n",
      "        Tokenize the English-French dataset.\n",
      "    \n",
      "    train_2d(trainer, steps=20, f_grad=None)\n",
      "        Optimize a 2D objective function with a customized trainer.\n",
      "    \n",
      "    train_batch_ch13(net, X, y, loss, trainer, devices)\n",
      "        Train for a minibatch with mutiple GPUs (defined in Chapter 13).\n",
      "    \n",
      "    train_ch11(trainer_fn, states, hyperparams, data_iter, feature_dim, num_epochs=2)\n",
      "        # Defined in file: ./chapter_optimization/minibatch-sgd.md\n",
      "    \n",
      "    train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices=[device(type='cuda', index=0)])\n",
      "        Train a model with mutiple GPUs (defined in Chapter 13).\n",
      "    \n",
      "    train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)\n",
      "        Train a model (defined in Chapter 3).\n",
      "    \n",
      "    train_ch6(net, train_iter, test_iter, num_epochs, lr, device)\n",
      "        Train a model with a GPU (defined in Chapter 6).\n",
      "    \n",
      "    train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False)\n",
      "        Train a model (defined in Chapter 8).\n",
      "    \n",
      "    train_concise_ch11(trainer_fn, hyperparams, data_iter, num_epochs=4)\n",
      "        # Defined in file: ./chapter_optimization/minibatch-sgd.md\n",
      "    \n",
      "    train_epoch_ch3(net, train_iter, loss, updater)\n",
      "        The training loop defined in Chapter 3.\n",
      "    \n",
      "    train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)\n",
      "        Train a net within one epoch (defined in Chapter 8).\n",
      "    \n",
      "    train_seq2seq(net, data_iter, lr, num_epochs, tgt_vocab, device)\n",
      "        Train a model for sequence to sequence.\n",
      "    \n",
      "    transpose lambda x, *args, **kwargs\n",
      "    \n",
      "    transpose_output(X, num_heads)\n",
      "        Reverse the operation of `transpose_qkv`.\n",
      "    \n",
      "    transpose_qkv(X, num_heads)\n",
      "        Transposition for parallel computation of multiple attention heads.\n",
      "    \n",
      "    truncate_pad(line, num_steps, padding_token)\n",
      "        Truncate or pad sequences.\n",
      "    \n",
      "    try_all_gpus()\n",
      "        Return all available GPUs, or [cpu(),] if no GPU exists.\n",
      "    \n",
      "    try_gpu(i=0)\n",
      "        Return gpu(i) if exists, otherwise return cpu().\n",
      "    \n",
      "    update_D(X, Z, net_D, net_G, loss, trainer_D)\n",
      "        Update discriminator.\n",
      "    \n",
      "    update_G(Z, net_D, net_G, loss, trainer_G)\n",
      "        Update generator.\n",
      "    \n",
      "    use_svg_display()\n",
      "        Use the svg format to display a plot in Jupyter.\n",
      "    \n",
      "    voc_colormap2label()\n",
      "        Build the mapping from RGB to class indices for VOC labels.\n",
      "    \n",
      "    voc_label_indices(colormap, colormap2label)\n",
      "        Map any RGB values in VOC labels to their class indices.\n",
      "    \n",
      "    voc_rand_crop(feature, label, height, width)\n",
      "        Randomly crop both feature and label images.\n",
      "    \n",
      "    zeros(...)\n",
      "        zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
      "        \n",
      "        Returns a tensor filled with the scalar value `0`, with the shape defined\n",
      "        by the variable argument :attr:`size`.\n",
      "        \n",
      "        Args:\n",
      "            size (int...): a sequence of integers defining the shape of the output tensor.\n",
      "                Can be a variable number of arguments or a collection like a list or tuple.\n",
      "        \n",
      "        Keyword args:\n",
      "            out (Tensor, optional): the output tensor.\n",
      "            dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
      "                Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
      "            layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
      "                Default: ``torch.strided``.\n",
      "            device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
      "                Default: if ``None``, uses the current device for the default tensor type\n",
      "                (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
      "                for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
      "            requires_grad (bool, optional): If autograd should record operations on the\n",
      "                returned tensor. Default: ``False``.\n",
      "        \n",
      "        Example::\n",
      "        \n",
      "            >>> torch.zeros(2, 3)\n",
      "            tensor([[ 0.,  0.,  0.],\n",
      "                    [ 0.,  0.,  0.]])\n",
      "        \n",
      "            >>> torch.zeros(5)\n",
      "            tensor([ 0.,  0.,  0.,  0.,  0.])\n",
      "\n",
      "DATA\n",
      "    DATA_HUB = {'SNLI': ('https://nlp.stanford.edu/projects/snli/snli_1.0....\n",
      "    DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'\n",
      "    VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat', '...\n",
      "    VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0], [0...\n",
      "    float32 = torch.float32\n",
      "    int32 = torch.int32\n",
      "\n",
      "FILE\n",
      "    d:\\programdata\\anaconda3\\envs\\torch\\lib\\site-packages\\d2l\\torch.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from d2l import torch as d2l\n",
    "help(d2l)\n",
    "d2l??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9175fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshape output shape：\t torch.Size([1, 1, 28, 28])\n",
      "Conv2d output shape：\t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape：\t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape：\t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape：\t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape：\t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape：\t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape：\t torch.Size([1, 400])\n",
      "Linear output shape：\t torch.Size([1, 120])\n",
      "Sigmoid output shape：\t torch.Size([1, 120])\n",
      "Linear output shape：\t torch.Size([1, 84])\n",
      "Sigmoid output shape：\t torch.Size([1, 84])\n",
      "Linear output shape：\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# LeNet(LeNet-5) 由两个部分组成：卷积编码器和全连接层密集块\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "class Reshape(torch.nn.Module):\n",
    "    def forward(self,x):\n",
    "        return x.view(-1,1,28,28) # 批量数自适应得到，通道数为1，图片为28X28\n",
    "    \n",
    "net = torch.nn.Sequential(\n",
    "        Reshape(), nn.Conv2d(1,6,kernel_size=5,padding=2),nn.Sigmoid(), # 6个5X5的卷积核\n",
    "        nn.AvgPool2d(2,stride=2),\n",
    "        nn.Conv2d(6,16,kernel_size=5),nn.Sigmoid(),\n",
    "        nn.AvgPool2d(kernel_size=2,stride=2),nn.Flatten(),\n",
    "        nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
    "        nn.Linear(120, 84), nn.Sigmoid(),\n",
    "        nn.Linear(84,10))   # 10个输出\n",
    "\n",
    "X = torch.rand(size=(1,1,28,28),dtype=torch.float32)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'output shape：\\t',X.shape) # 上一层的输出为这一层的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ae0166b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size=batch_size)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d3d8be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten Output shape:\t torch.Size([1, 6400])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 深度卷积神经网络 (AlexNet)\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),nn.ReLU(), # 数据集为fashion_mnist图片，所以输入通道为1，如果是Imagnet图片，则通道数应为3     \n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    nn.Conv2d(96,256,kernel_size=5,padding=2),nn.ReLU(), # 256为输出通道数\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    nn.Conv2d(256,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,256,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),nn.Flatten(),\n",
    "    nn.Linear(6400,4096),nn.ReLU(),nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096,4096),nn.ReLU(),nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096,10))\n",
    "\n",
    "X = torch.randn(1,1,224,224)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'Output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f40db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "# Fashion-MNIST图像的分辨率 低于ImageNet图像。将它们增加到224×224\n",
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,resize=224)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6fcee65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "ReLU Output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 384, 12, 12])\n",
      "Conv2d Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "ReLU Output shape:\t torch.Size([1, 256, 12, 12])\n",
      "MaxPool2d Output shape:\t torch.Size([1, 256, 5, 5])\n",
      "Flatten Output shape:\t torch.Size([1, 6400])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 4096])\n",
      "ReLU Output shape:\t torch.Size([1, 4096])\n",
      "Dropout Output shape:\t torch.Size([1, 4096])\n",
      "Linear Output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "# 深度卷积神经网络 (AlexNet)\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='TRUE'\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Conv2d(1,96,kernel_size=11,stride=4,padding=1),nn.ReLU(), # 数据集为fashion_mnist图片，所以输入通道为1，如果是Imagnet图片，则通道数应为3     \n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    nn.Conv2d(96,256,kernel_size=5,padding=2),nn.ReLU(), # 256为输出通道数\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "    nn.Conv2d(256,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,384,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.Conv2d(384,256,kernel_size=3,padding=1),nn.ReLU(),\n",
    "    nn.MaxPool2d(kernel_size=3,stride=2),nn.Flatten(),\n",
    "    nn.Linear(6400,4096),nn.ReLU(),nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096,4096),nn.ReLU(),nn.Dropout(p=0.5),\n",
    "    nn.Linear(4096,10))\n",
    "\n",
    "X = torch.randn(1,1,224,224)\n",
    "for layer in net:\n",
    "    X = layer(X)\n",
    "    print(layer.__class__.__name__,'Output shape:\\t', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d54720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNIST图像的分辨率 低于ImageNet图像。将它们增加到224×224\n",
    "batch_size = 128\n",
    "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size,resize=224)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20d151ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda:0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-50f4dc9170ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_ch6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0md2l\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtry_gpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\d2l\\torch.py\u001b[0m in \u001b[0;36mtrain_ch6\u001b[1;34m(net, train_iter, test_iter, num_epochs, lr, device)\u001b[0m\n\u001b[0;32m    494\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 496\u001b[1;33m             \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    497\u001b[0m             \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_hat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1846\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1848\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n",
       "<svg height=\"170.777344pt\" version=\"1.1\" viewBox=\"0 0 240.554688 170.777344\" width=\"240.554688pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       " <metadata>\r\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n",
       "   <cc:Work>\r\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n",
       "    <dc:date>2024-02-15T01:03:57.969668</dc:date>\r\n",
       "    <dc:format>image/svg+xml</dc:format>\r\n",
       "    <dc:creator>\r\n",
       "     <cc:Agent>\r\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\r\n",
       "     </cc:Agent>\r\n",
       "    </dc:creator>\r\n",
       "   </cc:Work>\r\n",
       "  </rdf:RDF>\r\n",
       " </metadata>\r\n",
       " <defs>\r\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n",
       " </defs>\r\n",
       " <g id=\"figure_1\">\r\n",
       "  <g id=\"patch_1\">\r\n",
       "   <path d=\"M 0 170.777344 \r\n",
       "L 240.554688 170.777344 \r\n",
       "L 240.554688 0 \r\n",
       "L 0 0 \r\n",
       "z\r\n",
       "\" style=\"fill:none;\"/>\r\n",
       "  </g>\r\n",
       "  <g id=\"axes_1\">\r\n",
       "   <g id=\"patch_2\">\r\n",
       "    <path d=\"M 30.103125 146.899219 \r\n",
       "L 225.403125 146.899219 \r\n",
       "L 225.403125 10.999219 \r\n",
       "L 30.103125 10.999219 \r\n",
       "z\r\n",
       "\" style=\"fill:#ffffff;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_1\">\r\n",
       "    <g id=\"xtick_1\">\r\n",
       "     <g id=\"line2d_1\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L 0 3.5 \r\n",
       "\" id=\"meaf0d37359\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#meaf0d37359\" y=\"146.899219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_1\">\r\n",
       "      <!-- 0.0 -->\r\n",
       "      <g transform=\"translate(22.151563 161.497656)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 31.78125 66.40625 \r\n",
       "Q 24.171875 66.40625 20.328125 58.90625 \r\n",
       "Q 16.5 51.421875 16.5 36.375 \r\n",
       "Q 16.5 21.390625 20.328125 13.890625 \r\n",
       "Q 24.171875 6.390625 31.78125 6.390625 \r\n",
       "Q 39.453125 6.390625 43.28125 13.890625 \r\n",
       "Q 47.125 21.390625 47.125 36.375 \r\n",
       "Q 47.125 51.421875 43.28125 58.90625 \r\n",
       "Q 39.453125 66.40625 31.78125 66.40625 \r\n",
       "z\r\n",
       "M 31.78125 74.21875 \r\n",
       "Q 44.046875 74.21875 50.515625 64.515625 \r\n",
       "Q 56.984375 54.828125 56.984375 36.375 \r\n",
       "Q 56.984375 17.96875 50.515625 8.265625 \r\n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \r\n",
       "Q 6.59375 17.96875 6.59375 36.375 \r\n",
       "Q 6.59375 54.828125 13.0625 64.515625 \r\n",
       "Q 19.53125 74.21875 31.78125 74.21875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-48\"/>\r\n",
       "        <path d=\"M 10.6875 12.40625 \r\n",
       "L 21 12.40625 \r\n",
       "L 21 0 \r\n",
       "L 10.6875 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-46\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_2\">\r\n",
       "     <g id=\"line2d_2\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.163125\" xlink:href=\"#meaf0d37359\" y=\"146.899219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_2\">\r\n",
       "      <!-- 0.2 -->\r\n",
       "      <g transform=\"translate(61.211563 161.497656)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 19.1875 8.296875 \r\n",
       "L 53.609375 8.296875 \r\n",
       "L 53.609375 0 \r\n",
       "L 7.328125 0 \r\n",
       "L 7.328125 8.296875 \r\n",
       "Q 12.9375 14.109375 22.625 23.890625 \r\n",
       "Q 32.328125 33.6875 34.8125 36.53125 \r\n",
       "Q 39.546875 41.84375 41.421875 45.53125 \r\n",
       "Q 43.3125 49.21875 43.3125 52.78125 \r\n",
       "Q 43.3125 58.59375 39.234375 62.25 \r\n",
       "Q 35.15625 65.921875 28.609375 65.921875 \r\n",
       "Q 23.96875 65.921875 18.8125 64.3125 \r\n",
       "Q 13.671875 62.703125 7.8125 59.421875 \r\n",
       "L 7.8125 69.390625 \r\n",
       "Q 13.765625 71.78125 18.9375 73 \r\n",
       "Q 24.125 74.21875 28.421875 74.21875 \r\n",
       "Q 39.75 74.21875 46.484375 68.546875 \r\n",
       "Q 53.21875 62.890625 53.21875 53.421875 \r\n",
       "Q 53.21875 48.921875 51.53125 44.890625 \r\n",
       "Q 49.859375 40.875 45.40625 35.40625 \r\n",
       "Q 44.1875 33.984375 37.640625 27.21875 \r\n",
       "Q 31.109375 20.453125 19.1875 8.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-50\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_3\">\r\n",
       "     <g id=\"line2d_3\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.223125\" xlink:href=\"#meaf0d37359\" y=\"146.899219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_3\">\r\n",
       "      <!-- 0.4 -->\r\n",
       "      <g transform=\"translate(100.271563 161.497656)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 37.796875 64.3125 \r\n",
       "L 12.890625 25.390625 \r\n",
       "L 37.796875 25.390625 \r\n",
       "z\r\n",
       "M 35.203125 72.90625 \r\n",
       "L 47.609375 72.90625 \r\n",
       "L 47.609375 25.390625 \r\n",
       "L 58.015625 25.390625 \r\n",
       "L 58.015625 17.1875 \r\n",
       "L 47.609375 17.1875 \r\n",
       "L 47.609375 0 \r\n",
       "L 37.796875 0 \r\n",
       "L 37.796875 17.1875 \r\n",
       "L 4.890625 17.1875 \r\n",
       "L 4.890625 26.703125 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-52\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_4\">\r\n",
       "     <g id=\"line2d_4\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.283125\" xlink:href=\"#meaf0d37359\" y=\"146.899219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_4\">\r\n",
       "      <!-- 0.6 -->\r\n",
       "      <g transform=\"translate(139.331563 161.497656)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 33.015625 40.375 \r\n",
       "Q 26.375 40.375 22.484375 35.828125 \r\n",
       "Q 18.609375 31.296875 18.609375 23.390625 \r\n",
       "Q 18.609375 15.53125 22.484375 10.953125 \r\n",
       "Q 26.375 6.390625 33.015625 6.390625 \r\n",
       "Q 39.65625 6.390625 43.53125 10.953125 \r\n",
       "Q 47.40625 15.53125 47.40625 23.390625 \r\n",
       "Q 47.40625 31.296875 43.53125 35.828125 \r\n",
       "Q 39.65625 40.375 33.015625 40.375 \r\n",
       "z\r\n",
       "M 52.59375 71.296875 \r\n",
       "L 52.59375 62.3125 \r\n",
       "Q 48.875 64.0625 45.09375 64.984375 \r\n",
       "Q 41.3125 65.921875 37.59375 65.921875 \r\n",
       "Q 27.828125 65.921875 22.671875 59.328125 \r\n",
       "Q 17.53125 52.734375 16.796875 39.40625 \r\n",
       "Q 19.671875 43.65625 24.015625 45.921875 \r\n",
       "Q 28.375 48.1875 33.59375 48.1875 \r\n",
       "Q 44.578125 48.1875 50.953125 41.515625 \r\n",
       "Q 57.328125 34.859375 57.328125 23.390625 \r\n",
       "Q 57.328125 12.15625 50.6875 5.359375 \r\n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \r\n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \r\n",
       "Q 6.984375 17.96875 6.984375 36.375 \r\n",
       "Q 6.984375 53.65625 15.1875 63.9375 \r\n",
       "Q 23.390625 74.21875 37.203125 74.21875 \r\n",
       "Q 40.921875 74.21875 44.703125 73.484375 \r\n",
       "Q 48.484375 72.75 52.59375 71.296875 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-54\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_5\">\r\n",
       "     <g id=\"line2d_5\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.343125\" xlink:href=\"#meaf0d37359\" y=\"146.899219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_5\">\r\n",
       "      <!-- 0.8 -->\r\n",
       "      <g transform=\"translate(178.391563 161.497656)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 31.78125 34.625 \r\n",
       "Q 24.75 34.625 20.71875 30.859375 \r\n",
       "Q 16.703125 27.09375 16.703125 20.515625 \r\n",
       "Q 16.703125 13.921875 20.71875 10.15625 \r\n",
       "Q 24.75 6.390625 31.78125 6.390625 \r\n",
       "Q 38.8125 6.390625 42.859375 10.171875 \r\n",
       "Q 46.921875 13.96875 46.921875 20.515625 \r\n",
       "Q 46.921875 27.09375 42.890625 30.859375 \r\n",
       "Q 38.875 34.625 31.78125 34.625 \r\n",
       "z\r\n",
       "M 21.921875 38.8125 \r\n",
       "Q 15.578125 40.375 12.03125 44.71875 \r\n",
       "Q 8.5 49.078125 8.5 55.328125 \r\n",
       "Q 8.5 64.0625 14.71875 69.140625 \r\n",
       "Q 20.953125 74.21875 31.78125 74.21875 \r\n",
       "Q 42.671875 74.21875 48.875 69.140625 \r\n",
       "Q 55.078125 64.0625 55.078125 55.328125 \r\n",
       "Q 55.078125 49.078125 51.53125 44.71875 \r\n",
       "Q 48 40.375 41.703125 38.8125 \r\n",
       "Q 48.828125 37.15625 52.796875 32.3125 \r\n",
       "Q 56.78125 27.484375 56.78125 20.515625 \r\n",
       "Q 56.78125 9.90625 50.3125 4.234375 \r\n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \r\n",
       "Q 19.734375 -1.421875 13.25 4.234375 \r\n",
       "Q 6.78125 9.90625 6.78125 20.515625 \r\n",
       "Q 6.78125 27.484375 10.78125 32.3125 \r\n",
       "Q 14.796875 37.15625 21.921875 38.8125 \r\n",
       "z\r\n",
       "M 18.3125 54.390625 \r\n",
       "Q 18.3125 48.734375 21.84375 45.5625 \r\n",
       "Q 25.390625 42.390625 31.78125 42.390625 \r\n",
       "Q 38.140625 42.390625 41.71875 45.5625 \r\n",
       "Q 45.3125 48.734375 45.3125 54.390625 \r\n",
       "Q 45.3125 60.0625 41.71875 63.234375 \r\n",
       "Q 38.140625 66.40625 31.78125 66.40625 \r\n",
       "Q 25.390625 66.40625 21.84375 63.234375 \r\n",
       "Q 18.3125 60.0625 18.3125 54.390625 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-56\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"xtick_6\">\r\n",
       "     <g id=\"line2d_6\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"225.403125\" xlink:href=\"#meaf0d37359\" y=\"146.899219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_6\">\r\n",
       "      <!-- 1.0 -->\r\n",
       "      <g transform=\"translate(217.451563 161.497656)scale(0.1 -0.1)\">\r\n",
       "       <defs>\r\n",
       "        <path d=\"M 12.40625 8.296875 \r\n",
       "L 28.515625 8.296875 \r\n",
       "L 28.515625 63.921875 \r\n",
       "L 10.984375 60.40625 \r\n",
       "L 10.984375 69.390625 \r\n",
       "L 28.421875 72.90625 \r\n",
       "L 38.28125 72.90625 \r\n",
       "L 38.28125 8.296875 \r\n",
       "L 54.390625 8.296875 \r\n",
       "L 54.390625 0 \r\n",
       "L 12.40625 0 \r\n",
       "z\r\n",
       "\" id=\"DejaVuSans-49\"/>\r\n",
       "       </defs>\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"matplotlib.axis_2\">\r\n",
       "    <g id=\"ytick_1\">\r\n",
       "     <g id=\"line2d_7\">\r\n",
       "      <defs>\r\n",
       "       <path d=\"M 0 0 \r\n",
       "L -3.5 0 \r\n",
       "\" id=\"m9fba1370bc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n",
       "      </defs>\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9fba1370bc\" y=\"146.899219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_7\">\r\n",
       "      <!-- 0.0 -->\r\n",
       "      <g transform=\"translate(7.2 150.698437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_2\">\r\n",
       "     <g id=\"line2d_8\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9fba1370bc\" y=\"119.719219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_8\">\r\n",
       "      <!-- 0.2 -->\r\n",
       "      <g transform=\"translate(7.2 123.518437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_3\">\r\n",
       "     <g id=\"line2d_9\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9fba1370bc\" y=\"92.539219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_9\">\r\n",
       "      <!-- 0.4 -->\r\n",
       "      <g transform=\"translate(7.2 96.338437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_4\">\r\n",
       "     <g id=\"line2d_10\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9fba1370bc\" y=\"65.359219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_10\">\r\n",
       "      <!-- 0.6 -->\r\n",
       "      <g transform=\"translate(7.2 69.158437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_5\">\r\n",
       "     <g id=\"line2d_11\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9fba1370bc\" y=\"38.179219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_11\">\r\n",
       "      <!-- 0.8 -->\r\n",
       "      <g transform=\"translate(7.2 41.978437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "    <g id=\"ytick_6\">\r\n",
       "     <g id=\"line2d_12\">\r\n",
       "      <g>\r\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m9fba1370bc\" y=\"10.999219\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "     <g id=\"text_12\">\r\n",
       "      <!-- 1.0 -->\r\n",
       "      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\r\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\r\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n",
       "      </g>\r\n",
       "     </g>\r\n",
       "    </g>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_3\">\r\n",
       "    <path d=\"M 30.103125 146.899219 \r\n",
       "L 30.103125 10.999219 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_4\">\r\n",
       "    <path d=\"M 225.403125 146.899219 \r\n",
       "L 225.403125 10.999219 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_5\">\r\n",
       "    <path d=\"M 30.103125 146.899219 \r\n",
       "L 225.403125 146.899219 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "   <g id=\"patch_6\">\r\n",
       "    <path d=\"M 30.103125 10.999219 \r\n",
       "L 225.403125 10.999219 \r\n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n",
       "   </g>\r\n",
       "  </g>\r\n",
       " </g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<Figure size 252x180 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr, num_epochs = 0.01, 10\n",
    "d2l.train_ch6(net,train_iter,test_iter,num_epochs,lr,d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94d0288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43af9da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d5fbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
